# Model Configuration
d_model: 128
num_heads: 4
d_ff: 512
num_layers: 2
max_seq_len: 128
dropout: 0.1
use_positional_encoding: true
model_type: encoder_decoder

# Training Configuration
batch_size: 32
learning_rate: 0.001
weight_decay: 0.01
num_epochs: 30
random_seed: 42

# Experiment Configuration
experiment_name: "seq2seq"
save_interval: 10