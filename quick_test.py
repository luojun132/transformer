import torch
import yaml
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from model import TransformerLM
from download_data import load_manual_data


def quick_model_test():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ"""
    print("=== å¿«é€Ÿæ¨¡å‹æµ‹è¯• ===")

    try:
        # åŠ è½½å°æ‰¹é‡æ•°æ®
        train_loader, val_loader, tokenizer = load_manual_data(
            batch_size=8,  # å°batch
            max_length=64  # çŸ­åºåˆ—
        )

        # æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
        model = TransformerLM(
            vocab_size=tokenizer.get_vocab_size(),
            d_model=128,
            num_heads=4,
            d_ff=256,  # è¾ƒå°çš„FFN
            num_layers=2,
            max_seq_len=64,
            dropout=0.1,
            use_positional_encoding=True
        )

        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ!")
        print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

        # æµ‹è¯•å‰å‘ä¼ æ’­
        for batch in train_loader:
            print(f"è¾“å…¥å½¢çŠ¶: {batch.shape}")

            # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
            input_ids = batch[:, :-1]
            target_ids = batch[:, 1:]

            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                logits = model(input_ids)
                print(f"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")

                # è®¡ç®—æŸå¤±
                criterion = torch.nn.CrossEntropyLoss(ignore_index=0)
                loss = criterion(
                    logits.reshape(-1, logits.size(-1)),
                    target_ids.reshape(-1)
                )
                print(f"åˆå§‹æŸå¤±: {loss.item():.4f}")

            break  # åªæµ‹è¯•ä¸€ä¸ªbatch

        print("âœ… æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡!")
        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç åŠŸèƒ½"""
    print("\n=== ä½ç½®ç¼–ç æµ‹è¯• ===")

    try:
        # æµ‹è¯•æœ‰ä½ç½®ç¼–ç 
        model_with_pe = TransformerLM(
            vocab_size=1000,
            d_model=64,
            num_heads=2,
            d_ff=128,
            num_layers=1,
            max_seq_len=32,
            dropout=0.1,
            use_positional_encoding=True
        )
        print("âœ… æœ‰ä½ç½®ç¼–ç æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•æ— ä½ç½®ç¼–ç 
        model_without_pe = TransformerLM(
            vocab_size=1000,
            d_model=64,
            num_heads=2,
            d_ff=128,
            num_layers=1,
            max_seq_len=32,
            dropout=0.1,
            use_positional_encoding=False
        )
        print("âœ… æ— ä½ç½®ç¼–ç æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randint(0, 1000, (2, 16))  # batch_size=2, seq_len=16

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_with_pe = model_with_pe(test_input)
            output_without_pe = model_without_pe(test_input)

            print(f"æœ‰ä½ç½®ç¼–ç è¾“å‡ºå½¢çŠ¶: {output_with_pe.shape}")
            print(f"æ— ä½ç½®ç¼–ç è¾“å‡ºå½¢çŠ¶: {output_without_pe.shape}")

            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸åŒ
            diff = torch.abs(output_with_pe - output_without_pe).mean()
            print(f"è¾“å‡ºå·®å¼‚: {diff.item():.6f}")

            if diff > 1e-6:
                print("âœ… ä½ç½®ç¼–ç æ­£å¸¸å·¥ä½œ!")
            else:
                print("âŒ ä½ç½®ç¼–ç å¯èƒ½æ²¡æœ‰ç”Ÿæ•ˆ")

        return True

    except Exception as e:
        print(f"âŒ ä½ç½®ç¼–ç æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_all_components():
    """æµ‹è¯•æ‰€æœ‰ç»„ä»¶"""
    print("=== å¼€å§‹å…¨é¢æµ‹è¯• ===")

    tests_passed = 0
    total_tests = 2

    # æµ‹è¯•1: åŸºç¡€æ¨¡å‹
    if quick_model_test():
        tests_passed += 1

    # æµ‹è¯•2: ä½ç½®ç¼–ç 
    if test_positional_encoding():
        tests_passed += 1

    print(f"\n=== æµ‹è¯•ç»“æœ: {tests_passed}/{total_tests} é€šè¿‡ ===")

    if tests_passed == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
        return False


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•å‡½æ•°ï¼Œä¸ä½¿ç”¨pytest
    test_all_components()